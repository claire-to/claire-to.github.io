[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBeyond the Worst-Case Analysis\n\n\n\n\n\n\nalgorithms\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJun 18, 2025\n\n\nClaire To\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Algorithm Analysis\n\n\n\n\n\n\nalgorithms\n\n\nteaching\n\n\n\n\n\n\n\n\n\nJun 16, 2025\n\n\nClaire To\n\n\n\n\n\n\n\n\n\n\n\n\nStaying Long Enough to Belong\n\n\n\n\n\n\nreflections\n\n\n\n\n\n\n\n\n\nMay 2, 2025\n\n\nClaire To\n\n\n\n\n\n\n\n\n\n\n\n\nNature, Nurture, and the Space Between\n\n\n\n\n\n\nreflections\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nClaire To\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Application Essay\n\n\n\n\n\n\nreflections\n\n\n\n\n\n\n\n\n\nDec 15, 2024\n\n\nClaire To\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/alg-analysis/index.html",
    "href": "posts/alg-analysis/index.html",
    "title": "Understanding Algorithm Analysis",
    "section": "",
    "text": "I’ve noticed that a lot of students – whether they’re currently taking algorithms or have already completed the course – often confuse cases with bounds when discussing algorithm analysis. I know I personally have had to wrestle with this concept myself for quite a while.\nMy hope is that this helps clear up some of that confusion and provides an intuitive, tangible overview of these concepts. It’s not meant to be a comprehensive or formal discussion, as there are already countless resources online, but rather a more approachable supplement for those who already have some background."
  },
  {
    "objectID": "posts/alg-analysis/index.html#case-analysis",
    "href": "posts/alg-analysis/index.html#case-analysis",
    "title": "Understanding Algorithm Analysis",
    "section": "Case Analysis",
    "text": "Case Analysis\nWhen analyzing how long an algorithm takes, we consider how it behaves in different cases, depending on the input.\n\nBest Case\nThe shortest possible time an algorithm takes on any input.\nFor example, when searching for a number in a list, the best case could be finding it at the first index. In insertion sort, the best case is when the array is already sorted.\nThis is less useful in practice, since it’s too optimistic and best case scenarios are unlikely to happen.\n\n\nWorst Case\nThe longest possible time an algorithm takes on any input.\nFor example, the worst case in searching for a number in a list is when the number is found at the very last position checked or not found in the list at all. In insertion sort, the worst case occurs when the array is reverse-sorted.\nThis is the most pessimistic, but also a reliable way to measure an algorithm’s performance because it guarantees that the algorithm will always run within a certain time.\n\n\nAverage Case\nThe expected runtime over all inputs, assuming a certain input distribution.\nThat means we average the running times of the algorithm over all possible inputs, weighted by their probability.\n\nProbability Distributions\n\nUniform distribution: Models all input where each instance is equally likely. Most common in average-case analysis of general algorithms.\nBernoulli distribution: Useful when inputs are sequences of binary events (success/failure). Most common in probabilistic or randomized algorithms.\nGeometric distribution: Models the number of trials until the first success. Most common in probabilistic or randomized algorithms."
  },
  {
    "objectID": "posts/alg-analysis/index.html#asymptotic-bounds",
    "href": "posts/alg-analysis/index.html#asymptotic-bounds",
    "title": "Understanding Algorithm Analysis",
    "section": "Asymptotic Bounds",
    "text": "Asymptotic Bounds\nIn algorithm analysis, we often use asymptotic bounds to describe how an algorithm’s running time grows as the input grows larger.\nYou can visualize this by plotting the runtime for increasing input sizes and comparing it to well-known functions like \\(n\\), \\(n \\log n\\), or \\(n^2\\). This is the core idea behind asymptotic comparison of growth rates.\n\nUpper Bound\nAn upper bound usually describes the maximum amount of work a specific algorithm requires to solve a problem.\nAn upper bound for a problem is established by providing an algorithm that achieves that bound. Thus, an algorithm’s upper bound also provides an upper bound for the problem it solves.\nUpper bounds are often proven using:\n\nDirect algorithm analysis\nRecurrence relations\nAmortized analysis\n\n\n\nLower Bound\nWhen discussing lower bounds, it’s important to distinguish between a bound on a specific algorithm and a bound on the inherent difficulty of a problem.\n\nFor a Specific Algorithm\nA lower bound for an algorithm indicates the minimum amount of work that specific algorithm requires to solve a problem in a given case, usually in the worst case.\nLower bounds for algorithms are often proven using:\n\nDirect algorithm analysis\n\n\n\nFor a Problem\nA lower bound for a problem indicates the minimum amount of work any correct algorithm requires to solve a problem, always in the worst case.\nThis is a much stronger statement about the fundamental, inherent difficulty of the problem itself. Lower bound proofs for problems establish the theoretical best performance achievable under a defined model of computation.\nFor example, searching for an element in an unsorted list requires \\(n\\) comparisions because we must check every element. Otherwise, we might miss the one we didn’t check, and that could’ve been the correct one.\nLower bounds for problems are often proven using:\n\nCounting arguments\n\nDecision trees\n\nAdversary arguments\n\nInformation theory\nReductions (P ?= NP)"
  },
  {
    "objectID": "posts/alg-analysis/index.html#asymptotic-notation",
    "href": "posts/alg-analysis/index.html#asymptotic-notation",
    "title": "Understanding Algorithm Analysis",
    "section": "Asymptotic Notation",
    "text": "Asymptotic Notation\nIn theoretical computer science, unless specified otherwise, asymptotic notation usually refers to the worst case. In practice, especially with randomized algorithms or heuristics, expected or average case complexity may be more relevant.\n\nBig O: \\(O(f(n))\\)\nAn upper bound that shows the algorithm \\(A\\)’s running time grows no faster than some function \\(f(n)\\) as the input size grows.\nThe function \\(f(n)\\)’s graph is asymptotically above or equal to algorithm \\(A\\)’s graph.\n\n\nBig Omega: \\(\\Omega(f(n))\\)\nA lower bound that shows the algorithm \\(A\\)’s running time grows no slower than some function \\(f(n)\\) as the input size grows.\nThe function \\(f(n)\\)’s graph is asymptotically below or equal to algorithm \\(A\\)’s graph.\n\n\nTheta: \\(\\Theta(f(n))\\)\nThis indicates a tight bound. In other words, for a specific algorithm, its upper and lower bounds are equivalent up to constant factors. This gives us a very precise understanding of how that algorithm’s runtime grows.\nThe function \\(f(n)\\)’s graph is asymptotically equivalent to algorithm \\(A\\)’s graph.\nSolving recurrance relations, such as by using the Master Theorem, gives us a tight bound on that algorithm’s performance.\nAn algorithm is considered asymptotically optimal for a problem when it has a tight bound that matches the problem’s proven lower bound, meaning we have found the fastest possible algorithm for that problem. If an algorithm’s worst-case upper bound matches the lower bound of the problem it solves, then it is implied that the algorithm has a tight bound.\n\n\nLittle o: \\(o(f(n))\\)\nSimilar to Big O, but shows the algorithm \\(A\\)’s running time is strictly slower than \\(f(n)\\).\nThe function \\(f(n)\\)’s graph is asymptotically above algorithm \\(A\\)’s graph.\nFor example, \\(n\\) grows strictly slower than \\(n \\log n\\).\n\n\nLittle omega: \\(\\omega(f(n))\\)\nSimilar to Big Omega, but shows the algorithm \\(A\\)’s running time is strictly faster than \\(f(n)\\).\nThe function \\(f(n)\\)’s graph is asymptotically below algorithm \\(A\\)’s graph.\nFor example, \\(n \\log n\\) grows strictly faster than \\(n\\).\n\nRemember, cases describe an algorithm’s performance on specific types of input, while asymptotic bounds and its notation describe its growth rate relative to increasing input size. I hope this helps clear up some common misconceptions!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Claire A. To",
    "section": "",
    "text": "Hello! I’m a PhD student in Computer Science at the University of California, Irvine in the Center for Algorithms and Theory of Computation. I’m honored to be advised by Dr. Michael Goodrich.\nContact: claire [dot] to [at] uci [dot] edu"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Claire A. To",
    "section": "",
    "text": "Hello! I’m a PhD student in Computer Science at the University of California, Irvine in the Center for Algorithms and Theory of Computation. I’m honored to be advised by Dr. Michael Goodrich.\nContact: claire [dot] to [at] uci [dot] edu"
  },
  {
    "objectID": "index.html#research",
    "href": "index.html#research",
    "title": "Claire A. To",
    "section": "Research",
    "text": "Research\nMy research lies within the field of Theoretical Computer Science. I naturally gravitate toward algorithms and data structures, particularly when applied to problems in computational geometry. My interest in research grew from my experiences in computer science education, which reshaped how I think about learning and inspired me to pursue a PhD."
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Claire A. To",
    "section": "Teaching",
    "text": "Teaching\nCS 161: Design and Analysis of Algorithms\nTerm(s): Fall 2025"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Claire A. To",
    "section": "Education",
    "text": "Education\nUniversity of California, Irvine\nPh.D in Computer Science | Sept 2025 - Present\nOrange Coast College\nA.A in Dance | Aug 2024 - Present\nUniversity of California, Irvine\nB.S in Computer Science | Sept 2023 - Jun 2025\nOrange Coast College\nA.A in Liberal Arts | Aug 2021 - May 2023\nOrange Coast College\nA.S in Computer Science | Aug 2020 - May 2023\nHuntington Beach High School\nHigh School Diploma | Aug 2017 - May 2021"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Entropy-Bounded Computational Geometry Made Easier and Sensitive to Sortedness\nDavid Eppstein, Michael T. Goodrich, Abraham M. Illickan, Claire A. To\nThe 37th Canadian Conference on Computational Geometry (CCCG 2025)\nWe study instance optimality–algorithms that achieve the performance of the best correct algorithm on every input, with respect to a measure. By leveraging input structure and sortedness, we design simple algorithms for classic computational geometry problems, including maxima set, convex hull, and visibility, and prove new bounds under a complexity measure sensitive to both properties.\nPaper | Poster | Slides (Complete)\n\n\n\nInvestigating the Capabilities of Generative AI in Solving Data Structures, Algorithms, and Computability Problems\nNero Li, Shahar Broner, Yubin Kim, Katrina Mizuo, Elijah Sauder, Claire To, Albert Wang, Ofek Gila, Michael Shindler\nThe 56th Technical Symposium on Computer Science Education (SIGCSE TS 2025)\nOur study evaluates the ability of generative AI models to solve advanced problems in data structures, algorithms, and computability. By testing 165 free-response questions across 16 theoretical computer science topics, we analyze the strengths and limitations of these models, highlighting their potential for education and automated tutoring.\nPaper | Slides\n\n\n\nCommon Strategy Patterns of Persuasion in a Mission Critical and Time Sensitive Task\nClaire To, Setareh Nasihati Gilani, David Traum\nThe 27th Workshop on the Semantics and Pragmatics of Dialogue (SemDial 2023 - MariLogue)\nOur research examines persuasion strategies in high-stakes, time-sensitive disaster relief interactions, analyzing how dialogue structure, speech acts, and urgency impact outcomes. By identifying patterns in communication, we explore how situational factors influence decision-making and persuasion effectiveness in crisis scenarios.\nPaper | Poster | Slides"
  },
  {
    "objectID": "posts/beyond-worst/index.html",
    "href": "posts/beyond-worst/index.html",
    "title": "Beyond the Worst-Case Analysis",
    "section": "",
    "text": "Worst-case analysis measures an algorithm by its performance on the hardest, adversarial input, often yielding an overly pessimistic result. This brings to light the discussion of beyond worst-case analysis: a more fine-grained approach that allows us to more accurately measure an algorithm’s performance.\nMy first theory research project, which I was fortunate to be part of during my undergrad, was inspired by the paper “Instance-Optimal Geometric Algorithms” by Peyman Afshani, Jérémy Barbay, and Timothy Chan. Here, I’ll briefly provide an overview of some of the core concepts."
  },
  {
    "objectID": "posts/beyond-worst/index.html#adaptive-analysis",
    "href": "posts/beyond-worst/index.html#adaptive-analysis",
    "title": "Beyond the Worst-Case Analysis",
    "section": "Adaptive Analysis",
    "text": "Adaptive Analysis\nAdaptive analysis measures an algorithm’s execution cost not only as a function of input size, but also with respect to other parameters that can more precisely capture the input’s inherent difficulty (e.g., output size).\nFor example, in computational geometry, there are well-known output-sensitive algorithms to compute 2D and 3D convex hulls in \\(O(n \\text{ log } h)\\), where \\(h\\) is the output size (i.e., the number of points on the hull). The fewer points there are on the hull, the faster the algorithm will run."
  },
  {
    "objectID": "posts/beyond-worst/index.html#instance-optimality",
    "href": "posts/beyond-worst/index.html#instance-optimality",
    "title": "Beyond the Worst-Case Analysis",
    "section": "Instance Optimality",
    "text": "Instance Optimality\nInstance optimality represents the ultimate form of adaptive analysis. An algorithm is instance-optimal if its cost is within a constant factor of the best possible algorithm’s cost on that exact same input.\nFor example, the 2D convex hull problem has a worst-case complexity of \\(\\Theta (n \\text{ log } n)\\) in the algebraic computation tree model (a common model for analyzing geometric algorithms where only comparisons and basic arithmetic are allowed), but algorithms exist that can solve this problem on certain input instances (e.g. sorted points) in \\(O(n)\\) time. An instance-optimal algorithm would need to match this runtime.\nLikewise, to sort a list of numbers, a list that is already sorted should run faster could be “sorted” in \\(O(n)\\) time. We only need to check if the list is already sorted then we can return it without any additional operations. In the comparison-based model (where algorithms use only element comparisons), an instance-optimal sorting algorithm would need to achieve \\(O(n)\\) time on already-sorted input if there exists some comparison-based algorithm that does so.\nGiven the strictness of this definition, the paper introduces two variants of instance optimality: order-oblivious and random-order settings.\nThese variations allow us to show instance optimality against a strong benchmark even when a universally ultimate optimal algorithm is a theoretical ideal that is unknown or unproven. In my project, we worked on improving upon and proving algorithms in the order-oblivious setting.\n\nOrder-Oblivious Setting\nAn algorithm \\(A\\) is considered instance-optimal in the order-oblivious setting if, for any given input set of elements, its worst-case runtime (across all possible permutations of that input) is within a constant factor of the best possible worst-case runtime achieved by any algorithm \\(A'\\) from the class \\(C\\) of all correct algorithms.\nBecause this compares the performance of algorithms \\(A\\) and \\(A’\\) on their own worst permutation (which may not be the same between them), this analysis ensures that the algorithm’s performance guarantee holds regardless of how the input elements are initially ordered. In other words, the analysis conceptually shuffles the input to find the hardest scenario for a given algorithm and input set, even if the algorithm itself does not randomize the input.\nThis implies that such an algorithm is also optimal output-sensitive and optimal adaptive with respect to any parameters that are independent of the input’s order (e.g., data spread, expected size, or relative positions of points).\n\n\nRandom-Order Setting\nAn algorithm \\(A\\) is considered instance-optimal in the random-order setting if, for any given input set of elements, its average runtime (across all possible permutations of that input) is within a constant factor of the best possible average runtime achieved by any algorithm \\(A'\\) from the class \\(C\\) of all correct algorithms.\nThe analysis implicitly creates a uniform probability distribution over all permutations of the input set and then calculates the algorithm’s average performance under this distribution.\nTherefore, this is a stronger definition than order-oblivious instance optimality because the average runtime across all permutations of that set will never be worse than the worst runtime across all permutations of that same input set. By averaging over all possible input permutations, this setting captures the algorithm’s expected performance, preventing the influence of either best-case or worst-case input ordering.\nThis setting is particularly relevant for analyzing algorithms that incorporate randomness. Algorithms that are instance-optimal in the random-order setting are competitive against randomized or randomized incremental algorithms (where input elements are randomly permuted as a preprocessing step) and traditional average-case optimality (which assumes an input probability distribution).\n\nIn this dynamic field of algorithm analysis, adaptive and instance-optimal analysis are only two out of the many powerful approaches. To delve deeper, Tim Roughgarden’s “Beyond the Worst-Case Analysis of Algorithms” course and textbook covers a number of modeling methods and frameworks for going beyond worst-case analysis. Ultimately, the goal of these techniques is to bridge the gap between theoretical guarantees and real-world performance."
  }
]