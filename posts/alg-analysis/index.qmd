---
title: "Understanding Algorithm Analysis"
author: "Claire To"
date: "2025-06-16"
categories: [algorithms]
title-block-banner: true
---

I’ve noticed that a lot of students -- whether they're currently taking algorithms or have already completed the course -- often confuse cases with bounds when discussing algorithm analysis. I know I personally have had to wrestle with this concept myself for quite a while. 

My hope is that this helps clear up some of that confusion and provides an intuitive, tangible overview of these concepts. It’s not meant to be a comprehensive or formal discussion, as there are already countless resources online, but rather a more approachable supplement for those who already have some background.

## Case Analysis

When analyzing how long an algorithm takes, we consider how it behaves in different cases, depending on the input.

### Best Case

The shortest possible time an algorithm takes on any input.

For example, when searching for a number in a list, the best case could be finding it at the first index. In insertion sort, the best case is when the array is already sorted.

This is less useful in practice, since it’s too optimistic and best case scenarios are unlikely to happen.

### Worst Case

The longest possible time an algorithm takes on any input.

For example, the worst case in searching for a number in a list is when the number is found at the very last position checked or not found in the list at all. In insertion sort, the worst case occurs when the array is reverse-sorted.

This is the most pessimistic, but also a reliable way to measure an algorithm’s performance because it guarantees that the algorithm will always run within a certain time.

### Average Case

The expected runtime over all inputs, assuming a certain input distribution. 

That means we average the running times of the algorithm over all possible inputs, weighted by their probability.

#### Probability Distributions

- **Uniform distribution:** Models all input where each instance is equally likely. Most common in average-case analysis of general algorithms.

- **Bernoulli distribution:** Useful when inputs are sequences of binary events (success/failure). Most common in probabilistic or randomized algorithms.

- **Geometric distribution:** Models the number of trials until the first success. Most common in probabilistic or randomized algorithms.

## Asymptotic Bounds

In algorithm analysis, we often use asymptotic bounds to describe how an algorithm's running time grows as the input grows larger.

You can visualize this by plotting the runtime for increasing input sizes and comparing it to well-known functions like $n$, $n \log n$, or $n^2$. This is the core idea behind asymptotic comparison of growth rates.

### Lower Bound

A lower bound proves that any correct algorithm must do at least a certain amount of work for a problem.

For example, searching for an element in an unsorted list requires $n$ comparisions because we must check every element. Otherwise, we might miss the one we didn’t check, and that could’ve been the correct one.

Lower bounds are often proven using:

- Counting arguments  
- Decision trees  
- Adversary arguments  

When we discuss lower bounds, we’re referring to the **minimum** amount of work that **any correct algorithm** must perform to solve the problem.

### Upper Bound

An upper bound means we’ve found some algorithm that solves the problem within a certain time.

Upper bounds are usually proven through:

- Explicit algorithm analysis  
- Reductions from known problems

When we discuss upper bounds, we’re referring to the **maximum** amount of work **a specific algorithm** requires to solve a problem.

## Asymptotic Notation

In theoretical computer science, unless specified otherwise, asymptotic notation usually refers to the **worst-case**. In practice, especially with randomized algorithms or heuristics, **expected or average-case** complexity may be more relevant.

### Big O: $O(f(n))$

An **upper bound** that shows the algorithm A’s running time grows **no faster** than some function $f(n)$ as the input size grows. 

The function $f(n)$ graphed is *above or equal* to algorithm A's graph.

### Big Omega: $\Omega(f(n))$

A **lower bound** that shows the algorithm A’s running time grows **no slower** than some function $f(n)$, as the input size grows. 

The function $f(n)$ graphed is *below or equal* to algorithm A's graph.

### Theta: $\Theta(f(n))$

This indicates a **tight bound**. In other words, the upper and lower bounds are equivalent up to constant factors. 

Thus, the algorithm is **asymptotically optimal**.

### Little o: $o(f(n))$

Similar to Big O, but shows the algorithm A’s running time is **strictly slower than** $f(n)$. 

The function $f(n)$ graphed is *strictly above* algorithm A's graph.

For example, $n$ grows strictly slower than $n \log n$.

### Little omega: $\omega(f(n))$

Similar to Big Omega, but shows the algorithm A’s running time is **strictly faster than** $f(n)$. 

The function $f(n)$ graphed is *strictly below* algorithm A's graph.

For example, $n \log n$ grows strictly faster than $n$.

---

In algorithm analysis, we use asymptotic notation to express bounds on runtime as a way to measure performance across different input scenarios. Worst-case analysis is most common in theory, while average-case and expected-case analyses are more often used in practice. I hope this helps clear up some common misconceptions!